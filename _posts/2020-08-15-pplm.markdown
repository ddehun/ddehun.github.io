---
title: "PLUG AND PLAY LANGUAGE MODELS: A SIMPLE APPROACH TO CONTROLLED TEXT GENERATION(ICLR2020) 논문 정리"
layout: post
date: 2020-08-15 12:00
image: /assets/images/markdown.jpg
headerImage: false
tag:
- ICLR2020
- LM
- ControlledGeneration
star: false
category: blog
author: ddehun
description: Paper Summary


---

## 논문 정보

- ICLR 2020
- CMS, Caltech / HKUST / Uber AI
- [Paper](https://arxiv.org/pdf/1912.02164.pdf)

---

## Introduction

GPT, T5와 같이 큰 규모의 데이터에서 사전학습 된 transformer 계열의 언어 모델은 좋은 언어 생성 능력을 보여줍니다. 하지만 생성하는 텍스트에 특정 attribute (e.g., sentiment, topic, personality)를 담고싶을 경우, 이에 대한 모델의 수정 혹은 fine-tuning 등 추가적인 학습을 필요로 합니다.

본 논문은 사전 학습된 언어모델을 통한 controlled text generation Task에 대해 "Plug-and-Play"라는 새로운 패러다임을 제안합니다. 이름에서 알 수 있듯, 언어 모델 자체의 파라미터는 수정하지 않은 채 특정 attribute를 가진 텍스트를 생성하는 것을 목표로 합니다. 이를 위해 사용자가 직접 정의한 bag-of-words 혹은 아주 가벼운 classifier를 추가하여, 사용자가 원하는 attribute를 생성하도록 가이드합니다.

언어 모델에 대한 추가적인 학습이 전혀 이루어지지 않는 상태에서, 한 개 이상의 attribute를 상대적으로 간단하게 결과에 반영할 수 있다는 장점을 가지고 있습니다.

## Controlled Text Generation

일반적인 Controlled generation은 `P(x|a)`를 모델링합니다. 이때 `x` 는 하나의 생성된 텍스트, `a` 는 긍정/부정과 같은 attribute를 의미합니다. 위의 식을 bayes rule를 통해 다시 바라보면, 특정 attribute에 대한 discriminator `P(a|x)`와 일반적인 generation model `P(x)`의 곱에서 sampling하는 형태로 볼 수 있습니다. 보다 자세한 예시는 아래 그림과 같습니다.

![table2](/assets/images/pplm/table2.png)

위 식에서  `P(x)`를 시퀀스 내 각 토큰들 사이의 chain rule 형태로 풀어보면 다음과 같습니다.

[1]: https://arxiv.org/pdf/1907.11692.pdf

